{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* The purpose of this notebook is to test different ML models in **predicting correct values** of a **dirty data**.\n",
    "* We test the impact on **keeping**/**removing inconsistencies samples** during the **training** phase on the ML model **repair** performence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jar to Python : https://www.jython.org/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as dt\n",
    "import training as tr\n",
    "import preprocessing as tp\n",
    "import model as m\n",
    "import evaluation as eva\n",
    "import utils\n",
    "# import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import csv\n",
    "import json # 2.0.9\n",
    "import config\n",
    "import datetime\n",
    "\n",
    "# import torch # 2.2.0+cu121\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEARNING ALGORITHM\n",
    "algos = config.classifiers #['ann', 'logistic regression l2', 'xgboost', 'multinomial bayesian', ]\n",
    "\n",
    "#LANGUAGE MODEL\n",
    "langs = config.transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: trials_population\n",
      "---------------------------\n",
      "feature inclusion\n",
      "---------------------------\n",
      "labels ['elderly', 'adults', 'adolescents', 'children', 'female', 'male', 'healthy_volunteers']\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/lab/Documents/rihem/improving-data-cleaning-with-unstructured-data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert_model_name = 'bert-base-multilingual-cased'\n",
    "bert_model_name = 'bert-base-uncased'\n",
    " \n",
    "# Set up parameters\n",
    "num_epochs = 8\n",
    "learning_rate = 2e-5\n",
    "sampling = False\n",
    "# nb of folds\n",
    "splits = 3\n",
    "\n",
    "#overconfidence compensators\n",
    "compensators = [.9,.97,.5]\n",
    "\n",
    "#path to csv files\n",
    "path = os.path.abspath(os.getcwd())\n",
    "\n",
    "# dataset names\n",
    "data_index = 1 #dt.Allergens\n",
    "dataset = config.datasets[data_index] \n",
    "partial_key = dataset['keys'][0]\n",
    "dataName = dataset['data_dir']\n",
    "labels = dataset['labels']\n",
    "feature = dataset['features'][0]\n",
    "lambdac = compensators[data_index]\n",
    "\n",
    "print('dataset:', dataName)\n",
    "print('---------------------------')\n",
    "print('feature', feature)\n",
    "print('---------------------------')\n",
    "print('labels', labels)\n",
    "print('---------------------------')\n",
    "\n",
    "#labels and features\n",
    "dataFolder, labels, features, keys = dt.get_datasetSchema(dataName)\n",
    "partial_key = keys[0]\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training best ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials_population\n"
     ]
    }
   ],
   "source": [
    "print(dataName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: tf-idf-xgboost\n",
      "transformer: xgboost classifier: tf-idf\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#MODEL\n",
    "model_name = m.get_best_ml(data_index)\n",
    "for l in langs:\n",
    "    if l['name'] in model_name:\n",
    "        lang = l\n",
    "        break\n",
    "for al in algos:\n",
    "    if al['name'] in model_name:\n",
    "        alg = al\n",
    "        break\n",
    "\n",
    "print('model:', model_name)\n",
    "print('transformer:', alg['name'], 'classifier:', lang['name'])\n",
    "print('--------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - parker True - constraints_check False\n",
      "relative path ./data/trials_population --before delete (19213, 16)\n",
      "--after delete (17944, 16)\n",
      "label encoders {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies related to: elderly 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 2.308319039451115, 1: 0.6382499407161489}\n",
      "imbalance ratio 0.276 unique_classes [0 1] class_counts [ 3498 12651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:46:45] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▍                                      | 1/7 [00:15<01:30, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elderly avg proba 0.8723912268877029 ece 68.8116294145584\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: adults 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 5.195945945945946, 1: 0.5532374100719425}\n",
      "imbalance ratio 0.106 unique_classes [0 1] class_counts [ 1554 14595]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:47:00] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▊                                | 2/7 [00:27<01:07, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adults avg proba 0.9096833354234695 ece 14.376107305288263\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: adolescents 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 0.6138903672166046, 1: 2.6950934579439254}\n",
      "imbalance ratio 0.228 unique_classes [0 1] class_counts [13153  2996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:47:13] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████▎                         | 3/7 [00:40<00:53, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adolescents avg proba 0.8598654255270958 ece 67.83170127868652\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: children 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 0.5931462572540953, 1: 3.1839511041009465}\n",
      "imbalance ratio 0.186 unique_classes [0 1] class_counts [13613  2536]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:47:26] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▋                   | 4/7 [00:53<00:39, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children avg proba 0.8646958211064338 ece 42.63611858710647\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: female 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 15.648255813953488, 1: 0.5165035501823066}\n",
      "imbalance ratio 0.033 unique_classes [0 1] class_counts [  516 15633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:47:39] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████▏            | 5/7 [01:04<00:24, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female avg proba 0.9579240399599075 ece 3.8574822042138264\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: male 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 8.776630434782609, 1: 0.5302055289250771}\n",
      "imbalance ratio 0.06 unique_classes [0 1] class_counts [  920 15229]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:47:49] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████▌      | 6/7 [01:15<00:12, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male avg proba 0.941824947297573 ece 6.817829430103298\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: healthy_volunteers 0\n",
      "dtrain, valid_data (17944, 16) (1795, 16)\n",
      "{0: 0.5137430807405994, 1: 18.69097222222222}\n",
      "imbalance ratio 0.027 unique_classes [0 1] class_counts [15717   432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:48:01] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1795, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [01:26<00:00, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthy_volunteers avg proba 0.926705960035324 ece 14.52439729962498\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "1 - parker False - constraints_check True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative path ./data/trials_population --before delete (17944, 16)\n",
      "--after delete (14607, 16)\n",
      "label encoders {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies related to: elderly 85\n",
      "dtrain, valid_data (14257, 16) (1426, 16)\n",
      "{0: 2.1166281755196303, 1: 0.6546428571428572}\n",
      "imbalance ratio 0.309 unique_classes [0 1] class_counts [3031 9800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:48:12] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1426, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▍                                      | 1/7 [00:11<01:06, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elderly avg proba 0.8781195706129074 ece 50.481734067201664\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: adults 29\n",
      "dtrain, valid_data (14503, 16) (1451, 16)\n",
      "{0: 4.488308115543329, 1: 0.5626832212450422}\n",
      "imbalance ratio 0.125 unique_classes [0 1] class_counts [ 1454 11598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:48:23] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1451, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▊                                | 2/7 [00:21<00:53, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adults avg proba 0.9228563058376312 ece 6.832053780555696\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: adolescents 59\n",
      "dtrain, valid_data (14379, 16) (1438, 16)\n",
      "{0: 0.5841910436980859, 1: 3.469436997319035}\n",
      "imbalance ratio 0.168 unique_classes [0 1] class_counts [11076  1865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:48:33] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1438, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████▎                         | 3/7 [00:32<00:42, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adolescents avg proba 0.9112786373496056 ece 28.859265433624387\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: children 18\n",
      "dtrain, valid_data (14516, 16) (1452, 16)\n",
      "{0: 0.5654432132963989, 1: 4.3201058201058204}\n",
      "imbalance ratio 0.131 unique_classes [0 1] class_counts [11552  1512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:48:44] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1452, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▋                   | 4/7 [00:42<00:31, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children avg proba 0.9289646196365356 ece 18.39736930280923\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: female 3\n",
      "dtrain, valid_data (14594, 16) (1460, 16)\n",
      "{0: 14.925, 1: 0.5173310225303293}\n",
      "imbalance ratio 0.035 unique_classes [0 1] class_counts [  440 12694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:48:55] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1460, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████▏            | 5/7 [00:51<00:20, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female avg proba 0.9608811229467392 ece 5.504823535680753\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: male 6\n",
      "dtrain, valid_data (14577, 16) (1458, 16)\n",
      "{0: 9.968844984802432, 1: 0.5264023754112832}\n",
      "imbalance ratio 0.053 unique_classes [0 1] class_counts [  658 12461]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:49:04] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1458, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████▌      | 6/7 [01:01<00:09,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male avg proba 0.9502085718512535 ece 4.473736330866764\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: healthy_volunteers 7\n",
      "dtrain, valid_data (14579, 16) (1458, 16)\n",
      "{0: 0.5140249157721539, 1: 18.325418994413408}\n",
      "imbalance ratio 0.028 unique_classes [0 1] class_counts [12763   358]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:49:13] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (1458, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [01:10<00:00, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthy_volunteers avg proba 0.9577457049489021 ece 6.138123796670698\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "recordFile ./results/trials_population/results_training_best_ml.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "strategy_results = {}\n",
    "allmodels = {}\n",
    "for i in range(2):\n",
    "    if i == 1:\n",
    "        _parker = \"\"\n",
    "        _with = \"with_constraints\"        \n",
    "        parker = False\n",
    "        constraints_check = True\n",
    "    else:\n",
    "        _parker = \"_parker\"\n",
    "        _with = \"with_parker\"        \n",
    "        parker = True\n",
    "        constraints_check = False\n",
    "    print(i, '- parker', parker, '- constraints_check', constraints_check)\n",
    "    all_results = {}\n",
    "    models = {}\n",
    "    \n",
    "#     start_time = datetime.datetime.now()\n",
    "    \n",
    "    # read training data and valid data\n",
    "    train_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_train.csv\")\n",
    "    train_data = dt.read_data_csv(dataset['data_dir'], 'train', parker) \n",
    "    #utils.load_df(dataset, train_file_path)\n",
    "    # encode non-numerical labels\n",
    "    dtrain, encoders = tp.preprocess(dataset, train_data)\n",
    "    print('label encoders', encoders)\n",
    "    \n",
    "#     feature_processing_time  = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # per_model\n",
    "    result_per_model = {} \n",
    "#     for label in tqdm(labels):\n",
    "    for label in tqdm(labels):\n",
    "        #timestamp\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        #per label       \n",
    "        result_per_label = {}\n",
    "        \n",
    "        # remove the inconsistent rows\n",
    "        dtrain = tr.get_cleaner_train_version(dataName, label, train_data, partial_key)\n",
    "        dtrain1, valid_data = train_test_split(dtrain, test_size=0.1)\n",
    "        iter = 0\n",
    "        while len(valid_data[label].unique()) != len(dtrain[label].unique()):\n",
    "            dtrain1, valid_data = train_test_split(dtrain, test_size=0.1)\n",
    "            iter += 1\n",
    "            print(iter + \"th iteration\")\n",
    "        print('dtrain, valid_data', dtrain.shape, valid_data.shape)\n",
    "\n",
    "        y = dtrain1[label].astype(int)\n",
    "        n_class, unique_classes, class_counts, ir = tp.get_class_stats(y)\n",
    "        class_weights = {c: len(y) / (count * n_class) for c, count in zip(unique_classes, class_counts)}\n",
    "        print(class_weights)\n",
    "        \n",
    "        batch_size = dtrain.shape[0]\n",
    "        # Loop through DataFrame in chunks without numpy\n",
    "        for i in range(0, len(dtrain1), batch_size):\n",
    "            if i + batch_size < len(dtrain): batch = dtrain1[i:i + batch_size]\n",
    "            else: batch = dtrain1[i:len(dtrain)]\n",
    "            \n",
    "            # first training\n",
    "            if i == 0:            \n",
    "                model, result_per_label = tr.clf_train(batch, dataset, label, alg, lang, config.root_seed, True)\n",
    "                models[label] = model\n",
    "            else:\n",
    "                print(f\"Batch {(i // batch_size) + 1}:\\n\")\n",
    "                model.fit(batch[feature], batch[label])\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        result_per_label['duration'] = (current_time - start_time).total_seconds()\n",
    "        \n",
    "        # save the model\n",
    "        file_model_name = f\"./models/_{label}_classifier_{model_name}_{_with}.pth\"\n",
    "        pickle.dump(model, open(file_model_name, \"wb\"))\n",
    "        \n",
    "        print()\n",
    "        # save the encoder for each label\n",
    "        if len(encoders)>0:\n",
    "            if len(encoders[label])>0: \n",
    "                result_per_label['encoder'] = encoders[label]\n",
    "        print()\n",
    "            \n",
    "        \n",
    "        # evalutae ML model\n",
    "        print('valid data', valid_data.shape)\n",
    "        #dtest = dt.read_test_csv(dataName, parker)\n",
    "        if alg['name'] != 'ann' and lang['name'] != 'bert':\n",
    "            if len(encoders)>0:\n",
    "                encoder = encoders[label]\n",
    "            else:\n",
    "                encoder = {}\n",
    "            ## load saved model\n",
    "            file_model_name = f\"./models/_{label}_classifier_{model_name}_{_with}.pth\"\n",
    "            with open(file_model_name, 'rb') as f: model = pickle.load(f)                 \n",
    "            \n",
    "            avg_confidence = []\n",
    "            eces = []\n",
    "            outputs = model.predict_proba(valid_data[feature].str.lower())\n",
    "            y_pred = model.predict(valid_data[feature].str.lower())\n",
    "            \n",
    "            y = valid_data[label].values #map(encoder) # encoded\n",
    "    \n",
    "            for i in range(len(set(y))):# iterate over the number of classes\n",
    "                probabilities = outputs[:, i]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "                ece, avg_confidence_in_bin = eva.expected_calibration_error(y, probabilities, i, n_bins=5)\n",
    "                \n",
    "                acf = eva.avg_conf_correct_pred(y, y_pred, probabilities, i)\n",
    "                avg_confidence.append(acf)\n",
    "\n",
    "      \n",
    "        #max_proba = [np.max(p) for p in outputs ]\n",
    "        # compensation for the overconfidence of the model\n",
    "        result_per_label['proba'] = lambdac * sum(avg_confidence)/len(avg_confidence)\n",
    "        print(label, 'avg proba', result_per_label['proba'], 'ece', ece)\n",
    "\n",
    "        result_per_model[label] = result_per_label # for specific model\n",
    "        all_results[model_name] = result_per_model\n",
    "        \n",
    "        print('+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++')\n",
    "        print()\n",
    "\n",
    "      \n",
    "    # sll records of training and evaluation \n",
    "    strategy_results[_with] = all_results\n",
    "    allmodels[_with] = models\n",
    "    \n",
    "strategy_results['lambda'] = lambdac    \n",
    "# save training and test accuracy\n",
    "recordFile = f\"./results/{dataset['data_dir']}/results_training_best_ml.json\"\n",
    "print('recordFile', recordFile)\n",
    "#with open(recordFile, \"w\") as outfile: json.dump(strategy_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'with_parker': {'tf-idf-xgboost': {'elderly': {'ir': 0.276,\n",
       "    'duration': 14.546749,\n",
       "    'proba': 0.8723912268877029},\n",
       "   'adults': {'ir': 0.106, 'duration': 11.971939, 'proba': 0.9096833354234695},\n",
       "   'adolescents': {'ir': 0.228,\n",
       "    'duration': 12.777874,\n",
       "    'proba': 0.8598654255270958},\n",
       "   'children': {'ir': 0.186,\n",
       "    'duration': 12.418311,\n",
       "    'proba': 0.8646958211064338},\n",
       "   'female': {'ir': 0.033, 'duration': 10.136167, 'proba': 0.9579240399599075},\n",
       "   'male': {'ir': 0.06, 'duration': 11.017528, 'proba': 0.941824947297573},\n",
       "   'healthy_volunteers': {'ir': 0.027,\n",
       "    'duration': 10.198905,\n",
       "    'proba': 0.926705960035324}}},\n",
       " 'with_constraints': {'tf-idf-xgboost': {'elderly': {'ir': 0.309,\n",
       "    'duration': 10.715564,\n",
       "    'proba': 0.8781195706129074},\n",
       "   'adults': {'ir': 0.125, 'duration': 9.994932, 'proba': 0.9228563058376312},\n",
       "   'adolescents': {'ir': 0.168,\n",
       "    'duration': 10.13669,\n",
       "    'proba': 0.9112786373496056},\n",
       "   'children': {'ir': 0.131,\n",
       "    'duration': 10.085442,\n",
       "    'proba': 0.9289646196365356},\n",
       "   'female': {'ir': 0.035, 'duration': 8.8437, 'proba': 0.9608811229467392},\n",
       "   'male': {'ir': 0.053, 'duration': 9.159729, 'proba': 0.9502085718512535},\n",
       "   'healthy_volunteers': {'ir': 0.028,\n",
       "    'duration': 8.52979,\n",
       "    'proba': 0.9577457049489021}}},\n",
       " 'lambda': 0.97}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordFile = f\"./results/{dataset['data_dir']}/results_training_best_ml.json\"\n",
    "with open(recordFile, \"w\") as outfile: json.dump(strategy_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative path ./data/trials_population --before delete (1269, 23)\n",
      "--after delete (1269, 23)\n",
      "(1269, 23)\n",
      "+++++++++++++++++++++Start+++++++++++++++++++++++++++++\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 23 repairs 29 errors 34\n",
      " th 0.88\n",
      "elderly stats: PRECISION, RECALL, F1 (0.79, 0.68, 0.73)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 10 repairs 19 errors 12\n",
      " th 0.92\n",
      "adults stats: PRECISION, RECALL, F1 (0.53, 0.83, 0.65)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 207 repairs 218 errors 210\n",
      " th 0.91\n",
      "adolescents stats: PRECISION, RECALL, F1 (0.95, 0.99, 0.97)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 208 repairs 222 errors 217\n",
      " th 0.93\n",
      "children stats: PRECISION, RECALL, F1 (0.94, 0.96, 0.95)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 1 repairs 28 errors 1\n",
      " th 0.96\n",
      "female stats: PRECISION, RECALL, F1 (0.04, 1.0, 0.08)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 1 repairs 19 errors 5\n",
      " th 0.95\n",
      "male stats: PRECISION, RECALL, F1 (0.05, 0.2, 0.08)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 17 repairs 17 errors 17\n",
      " th 0.96\n",
      "healthy_volunteers stats: PRECISION, RECALL, F1 (1.0, 1.0, 1.0)\n",
      "correct_repairs, repairs, errors 472 692 496\n",
      "precision 0.68 recall 0.95\n",
      "recall 0.95\n",
      "F1 0.792638036809816\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "dtest = dt.read_test_csv(dataName, parker)\n",
    "dtest1 = dtest.copy()\n",
    "print(dtest1.shape)\n",
    "print('+++++++++++++++++++++Start+++++++++++++++++++++++++++++')\n",
    "\n",
    "for a in labels:\n",
    "    # test repaired by parker do not have the following columns: need to fix it!!\n",
    "    if a + '_gs'not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dt.read_gs_csv(dataName)[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_gs'))\n",
    "    # confidence score for each attribute\n",
    "    conf_score = round(strategy_results[_with][model_name][a]['proba'],2)\n",
    "    ## load saved model\n",
    "    file_model_name = f\"./models/_{a}_classifier_{model_name}_{_with}.pth\"\n",
    "    with open(file_model_name, 'rb') as f: model = pickle.load(f) \n",
    "    #model = models[label]\n",
    "\n",
    "    # get the encoder if exists and encode y_orig  y_gs\n",
    "    enc = {}\n",
    "    enc, y_orig, y_gs = tp.encode(encoders, a, dtest1)\n",
    "    print(\"------ done encoding ----------\")      \n",
    "    \n",
    "    # predict the values for the labels to be repaired\n",
    "    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dtest1, a, dataset, enc)\n",
    "    print(\"------ done predicting ----------\")\n",
    "\n",
    "    if a + '_orig' not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dtest1[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_orig')) \n",
    "        print('current columns:', dtest1.columns)\n",
    "\n",
    "    # evaluate on ground truth\n",
    "    y_repair = eva.assign_repair(outputs, y_orig.values, y_pred, conf_score)\n",
    "    # stats\n",
    "    correct_repair, repair, errors = eva.get_stats(y_repair, y_orig.values, y_gs.values)\n",
    "    # metrics\n",
    "    metrics = eva.get_metrics(y_repair, y_orig.values, y_gs.values)\n",
    "    print(' th', conf_score)\n",
    "    print(a, 'stats: PRECISION, RECALL, F1', metrics)\n",
    "\n",
    "    #dtest1[a] = y_pred\n",
    "\n",
    "crs, rs, es = eva.get_all_stats(dtest1, labels)\n",
    "print('correct_repairs, repairs, errors', crs, rs, es)\n",
    "print('precision', round(crs/rs,2), 'recall', round(crs/es,2))\n",
    "if es !=0: \n",
    "    print('recall', round(crs/es,2))\n",
    "    print('F1', 2 * round(crs/rs,2) * round(crs/es,2)/(round(crs/rs,2) + round(crs/es,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "adults       1.0\n",
      "adults_gs    0.0\n",
      "Name: 19, dtype: float64\n",
      "1  Written informed consent 2  Pediatric patients 5  17 years old with clinically stable chronic renal anemia 3  Hemodialysis treatment for at least 8 weeks 4  Body weight   10 kg 5  Adequate hemodialysis  URR of   65  or Kt V  1 2 for patients on thrice weekly HD  Patients with fewer or with more HD sessions per week should have a weekly Kt V   3 6 6  Baseline pre dialysis Hb concentration 10 0   12 0 g dL determined from the mean of weekly Hb values measured between weeks  2 to  1 7  Intravenous maintenance epoetin alfa  epoetin beta  or darbepoetin alfa with same dosing interval for at least 8 weeks before screening 8  Stable maintenance epoetin alfa  epoetin beta  or darbepoetin alfa treatment with no weekly dose change   25   increase or decrease  during the 2 weeks of screening  Patients who had been previously treated by the sc route could only participate if they have been receiving their ESA by the iv route for at least 8 weeks before screening  9  Adequate iron status defined as serum ferritin   100 ng mL or TSAT   20   or percentage of hypochromic red cells  10    mean of two values measured during screening\n",
      "2007-007758-70\n",
      "19    1\n",
      "Name: adults, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "a = random.choice(labels)\n",
    "df1 = dtest.copy()\n",
    "diff = df1[df1[a] != df1[a + '_gs']][[a,a+'_gs']]\n",
    "i = random.choice(diff.index)\n",
    "if diff.shape[0] > 0: \n",
    "    i = random.choice(diff.index)\n",
    "    print(i)\n",
    "    print(diff.loc[i])\n",
    "    print(df1.loc[i, feature])\n",
    "    print(df1.loc[i, partial_key])\n",
    "    print(dtest[dtest[partial_key]== df1.loc[i, partial_key]][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation textual field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = m.get_best_ml(data_index)\n",
    "for l in langs:\n",
    "    if l['name'] in model_name:\n",
    "        lang = l\n",
    "        break\n",
    "transformer = lang\n",
    "trans = transformer[\"fn\"](**transformer[\"fixed_params\"])\n",
    "\n",
    "features = dataset['features'][0]\n",
    "\n",
    "if data_index == 0: ratio = 0.35\n",
    "else: ratio = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate correlation between each text's embedded vector and the label values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies related to: arms 0\n",
      "data shape initial: 57928\n",
      "data shape now: 20275\n",
      "to save in ./features/_arms_top_features_tf-idf_with_constraints.csv\n",
      "arms -0.030639067284978994\n",
      "inconsistencies related to: open 76\n",
      "data shape initial: 57477\n",
      "data shape now: 20117\n",
      "to save in ./features/_open_top_features_tf-idf_with_constraints.csv\n",
      "open 0.007942268558438414\n",
      "inconsistencies related to: double_blind 21\n",
      "data shape initial: 57330\n",
      "data shape now: 20066\n",
      "to save in ./features/_double_blind_top_features_tf-idf_with_constraints.csv\n",
      "double_blind 0.00016379781110094325\n",
      "inconsistencies related to: single_blind 0\n",
      "data shape initial: 57330\n",
      "data shape now: 20066\n",
      "to save in ./features/_single_blind_top_features_tf-idf_with_constraints.csv\n",
      "single_blind 0.0558260560638072\n",
      "inconsistencies related to: controlled 0\n",
      "data shape initial: 57242\n",
      "data shape now: 20035\n",
      "to save in ./features/_controlled_top_features_tf-idf_with_constraints.csv\n",
      "controlled -0.03526732440204452\n",
      "inconsistencies related to: parallel_group 0\n",
      "data shape initial: 56596\n",
      "data shape now: 19809\n",
      "to save in ./features/_parallel_group_top_features_tf-idf_with_constraints.csv\n",
      "parallel_group -0.0022578654608780393\n",
      "inconsistencies related to: crossover 0\n",
      "data shape initial: 56596\n",
      "data shape now: 19809\n",
      "to save in ./features/_crossover_top_features_tf-idf_with_constraints.csv\n",
      "crossover 0.050045870015061326\n",
      "inconsistencies related to: randomised 0\n",
      "data shape initial: 56596\n",
      "data shape now: 19809\n",
      "to save in ./features/_randomised_top_features_tf-idf_with_constraints.csv\n",
      "randomised -0.03193576905331011\n",
      "+++++++++++++++++++++NEXT STRATEGY+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: arms 0\n",
      "data shape initial: 63829\n",
      "data shape now: 22340\n",
      "to save in ./features/_arms_top_features_tf-idf_with_parker.csv\n",
      "arms -0.025792792400525413\n",
      "inconsistencies related to: open 0\n",
      "data shape initial: 63829\n",
      "data shape now: 22340\n",
      "to save in ./features/_open_top_features_tf-idf_with_parker.csv\n",
      "open 0.00019104624919180808\n",
      "inconsistencies related to: double_blind 0\n",
      "data shape initial: 63829\n",
      "data shape now: 22340\n",
      "to save in ./features/_double_blind_top_features_tf-idf_with_parker.csv\n",
      "double_blind 0.007213562382242944\n",
      "inconsistencies related to: single_blind 0\n",
      "data shape initial: 63829\n",
      "data shape now: 22340\n",
      "to save in ./features/_single_blind_top_features_tf-idf_with_parker.csv\n",
      "single_blind 0.05377601558699943\n",
      "inconsistencies related to: controlled 0\n",
      "data shape initial: 63700\n",
      "data shape now: 22295\n",
      "to save in ./features/_controlled_top_features_tf-idf_with_parker.csv\n",
      "controlled -0.02391154944746294\n",
      "inconsistencies related to: parallel_group 0\n",
      "data shape initial: 63700\n",
      "data shape now: 22295\n",
      "to save in ./features/_parallel_group_top_features_tf-idf_with_parker.csv\n",
      "parallel_group 0.0063604104198995\n",
      "inconsistencies related to: crossover 0\n",
      "data shape initial: 63700\n",
      "data shape now: 22295\n",
      "to save in ./features/_crossover_top_features_tf-idf_with_parker.csv\n",
      "crossover 0.04827743531838879\n",
      "inconsistencies related to: randomised 0\n",
      "data shape initial: 63700\n",
      "data shape now: 22295\n",
      "to save in ./features/_randomised_top_features_tf-idf_with_parker.csv\n",
      "randomised -0.019712395214034263\n",
      "+++++++++++++++++++++NEXT STRATEGY+++++++++++++++++++++++++++++\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>chi2_score_randomised</th>\n",
       "      <th>p_values_randomised</th>\n",
       "      <th>sample_count_randomised</th>\n",
       "      <th>correlation_to_randomised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16161</th>\n",
       "      <td>label</td>\n",
       "      <td>264.242174</td>\n",
       "      <td>2.041041e-59</td>\n",
       "      <td>14512</td>\n",
       "      <td>-0.121358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21118</th>\n",
       "      <td>open</td>\n",
       "      <td>247.309612</td>\n",
       "      <td>1.002262e-55</td>\n",
       "      <td>15767</td>\n",
       "      <td>-0.051725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10151</th>\n",
       "      <td>extension</td>\n",
       "      <td>202.313030</td>\n",
       "      <td>6.532677e-46</td>\n",
       "      <td>3712</td>\n",
       "      <td>-0.081928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29585</th>\n",
       "      <td>term</td>\n",
       "      <td>194.575435</td>\n",
       "      <td>3.189137e-44</td>\n",
       "      <td>5661</td>\n",
       "      <td>0.103522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17105</th>\n",
       "      <td>long</td>\n",
       "      <td>192.216250</td>\n",
       "      <td>1.043725e-43</td>\n",
       "      <td>4345</td>\n",
       "      <td>-0.057023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18506</th>\n",
       "      <td>migalastat</td>\n",
       "      <td>9.719923</td>\n",
       "      <td>1.822812e-03</td>\n",
       "      <td>11</td>\n",
       "      <td>0.069593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18574</th>\n",
       "      <td>minitablets</td>\n",
       "      <td>9.693017</td>\n",
       "      <td>1.849696e-03</td>\n",
       "      <td>16</td>\n",
       "      <td>0.080634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23638</th>\n",
       "      <td>preceding</td>\n",
       "      <td>9.677989</td>\n",
       "      <td>1.864887e-03</td>\n",
       "      <td>58</td>\n",
       "      <td>-0.036602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>antecedent</td>\n",
       "      <td>9.585031</td>\n",
       "      <td>1.961701e-03</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.045874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13325</th>\n",
       "      <td>ibrutinib</td>\n",
       "      <td>9.579260</td>\n",
       "      <td>1.967877e-03</td>\n",
       "      <td>216</td>\n",
       "      <td>-0.036649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  chi2_score_randomised  p_values_randomised  \\\n",
       "16161        label             264.242174         2.041041e-59   \n",
       "21118         open             247.309612         1.002262e-55   \n",
       "10151    extension             202.313030         6.532677e-46   \n",
       "29585         term             194.575435         3.189137e-44   \n",
       "17105         long             192.216250         1.043725e-43   \n",
       "...            ...                    ...                  ...   \n",
       "18506   migalastat               9.719923         1.822812e-03   \n",
       "18574  minitablets               9.693017         1.849696e-03   \n",
       "23638    preceding               9.677989         1.864887e-03   \n",
       "1729    antecedent               9.585031         1.961701e-03   \n",
       "13325    ibrutinib               9.579260         1.967877e-03   \n",
       "\n",
       "       sample_count_randomised  correlation_to_randomised  \n",
       "16161                    14512                  -0.121358  \n",
       "21118                    15767                  -0.051725  \n",
       "10151                     3712                  -0.081928  \n",
       "29585                     5661                   0.103522  \n",
       "17105                     4345                  -0.057023  \n",
       "...                        ...                        ...  \n",
       "18506                       11                   0.069593  \n",
       "18574                       16                   0.080634  \n",
       "23638                       58                  -0.036602  \n",
       "1729                        31                  -0.045874  \n",
       "13325                      216                  -0.036649  \n",
       "\n",
       "[192 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def count_samples(feature_words, data, feature_column):\n",
    "    c = []\n",
    "    for i, w in feature_words.iterrows():\n",
    "        c.append(data[data[feature_column].str.contains(w['feature'], case=False)].shape[0])\n",
    "    return c\n",
    "for parker in [False, True]:\n",
    "    if not parker:\n",
    "        _parker = \"\"\n",
    "        _with = \"with_constraints\"\n",
    "    else:\n",
    "        _parker = \"_parker\"\n",
    "        _with = 'with_parker'\n",
    "    # read training data and test data\n",
    "    train_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_train.csv\")\n",
    "    test_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_test.csv\")\n",
    "    train_data = utils.load_df(dataset, train_file_path)\n",
    "    test_data = utils.load_df(dataset, test_file_path)\n",
    "        \n",
    "    # encode non-numerical labels\n",
    "    dtrain1, encoders = tp.preprocess(dataset, train_data)\n",
    "    for a in labels:\n",
    "        dtrain1 = tr.get_cleaner_train_version(dataName, a, dtrain1, partial_key)\n",
    "        print('data shape initial:', dtrain1.shape[0])\n",
    "        rd_samples = random.sample(sorted(dtrain1.index), round(dtrain1.shape[0] * ratio))     \n",
    "        dtrain = dtrain1[dtrain1.index.isin(rd_samples)]\n",
    "        print('data shape now:', dtrain.shape[0])\n",
    "        \n",
    "        X = dtrain[features].str.lower()\n",
    "        X_trans = trans.fit_transform(X)\n",
    "        \n",
    "        y = dtrain[a].astype(int)\n",
    "        chi2_scores, p_values = chi2(X_trans, y)\n",
    "        feature_scores = pd.DataFrame(\n",
    "         {'feature': trans.get_feature_names_out(), f'chi2_score_{a}': chi2_scores, f'p_values_{a}': p_values})\n",
    "    \n",
    "        feature_scores = feature_scores.sort_values(by=f'chi2_score_{a}', ascending=False)\n",
    "        feature_scores = feature_scores[feature_scores[f'p_values_{a}'] < .002].copy()\n",
    "        feature_scores[f'sample_count_{a}'] = count_samples(feature_scores, dtrain1, features)\n",
    "        file_top_features = f\"./features/_{a}_top_features_{lang['name']}_{_with}.csv\"\n",
    "        print('to save in', file_top_features)\n",
    "        \n",
    "        top_features = feature_scores.copy()\n",
    "    \n",
    "        correlations = []\n",
    "        for i in range(X_trans.toarray().shape[1]):\n",
    "            if i in top_features.index:\n",
    "                feature_values = X_trans.toarray()[:, i]\n",
    "                corr,s  = pearsonr(feature_values, y)\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        # compute the average correlation\n",
    "        correlations = correlations #, [c for c in correlations if c > 0]\n",
    "        average_correlation =  np.nanmean(correlations)\n",
    "        print(a, average_correlation)\n",
    "        top_features[f'correlation_to_{a}'] = correlations\n",
    "        top_features.to_csv(file_top_features, quoting=csv.QUOTE_NONNUMERIC, index=False)\n",
    "    \n",
    "    print('+++++++++++++++++++++NEXT STRATEGY+++++++++++++++++++++++++++++')\n",
    "    print()\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and repair data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "strategy_results = {}\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        _with = \"with_constraints\"        \n",
    "        parker = False\n",
    "        constraints_check = True\n",
    "    else:\n",
    "        _with = \"without_constraints\"        \n",
    "        parker = False\n",
    "        constraints_check = False\n",
    "    print(i, '- constraints_check', constraints_check)\n",
    "    all_results = {}\n",
    "    decoders = {}\n",
    "    \n",
    "    _parker = \"\"\n",
    "    if parker: _parker = \"_parker\"\n",
    "\n",
    "    # read training data and test data\n",
    "    train_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_train.csv\")\n",
    "    test_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_test.csv\")\n",
    "    train_data = utils.load_df(dataset, train_file_path)\n",
    "    test_data = utils.load_df(dataset, test_file_path)\n",
    "    \n",
    "    # encode non-numerical labels\n",
    "    train_data, encoders = tp.preprocess(dataset, train_data)\n",
    "    dpredicted = test_data.copy()\n",
    "\n",
    "    # nlp transformer\n",
    "    for lang in langs:\n",
    "        # classifiers\n",
    "        for alg in algos:\n",
    "            # set a dataframe as repaired data\n",
    "            dpredicted = test_data.copy()\n",
    "            model_name = m.get_modelName(lang['name'], alg['name'])            \n",
    "            print(model_name)\n",
    "            print('---------------------------')\n",
    "\n",
    "            # skip to next model\n",
    "            if (alg['name'] == 'ann' and lang['name'] != 'bert') or (alg['name'] != 'ann' and lang['name'] == 'bert'):\n",
    "                continue\n",
    "            \n",
    "            result_per_model = {}\n",
    "            for label in labels:\n",
    "                file_model_name = f\"./models/_{label}_classifier_{model_name}_{_with}.pth\"\n",
    "                result_per_label = {}\n",
    "                                \n",
    "                # get a cleaner version of the training dataset\n",
    "                if  constraints_check:\n",
    "                    dtrain = tr.get_cleaner_train_version(dataName, label, train_data, partial_key)\n",
    "                \n",
    "                n_class, unique_classes, class_counts, ir = tp.get_class_stats(dtrain[label].values)\n",
    "                \n",
    "                # ML training\n",
    "                if alg['name'] != 'ann' and lang['name'] != 'bert':\n",
    "                    model, result_per_label = tr.clf_train(dtrain, dataset, label, alg, lang, config.root_seed, False)\n",
    "                    # save the ML-based pipeline\n",
    "                    pickle.dump(model, open(file_model_name, \"wb\"))                \n",
    "\n",
    "                # ML model based on BERT and Neural Network\n",
    "                if alg['name'] == 'ann' and lang['name'] == 'bert':\n",
    "                    model, epochs = nn.NN_train(dtrain, label, ir, unique_classes, bert_model_name, learning_rate, num_epochs, features, config.root_seed)\n",
    "                    result_per_label['epochs'] = epochs\n",
    "                    # save the ML-based pipeline\n",
    "                    torch.save(model, file_model_name)\n",
    "                    result_per_label['imbalance ratio'] = ir\n",
    "                \n",
    "                \n",
    "                # evalutae ML model\n",
    "                print('test data', test_data.shape, dpredicted.shape)\n",
    "                if alg['name'] != 'ann' and lang['name'] != 'bert':\n",
    "                    if len(encoders)>0:\n",
    "                        encoder = encoders[label]\n",
    "                    else:\n",
    "                        encoder = {}\n",
    "                    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dpredicted, label, dataset, encoder)\n",
    "                    #dpredicted[label + '_orig'] = test_data[label].values \n",
    "                    #dpredicted[label] = y_pred\n",
    "                    print('dpredicted.columns', dpredicted.columns)\n",
    "                    # evaluate the accuracy of the predicted repairs                \n",
    "                    result_per_label['test accuracy'] = accuracy\n",
    "                    print(label, f\"Test Accuracy: {accuracy:.4f}\")                     \n",
    "                if alg['name'] == 'ann' and lang['name'] == 'bert': \n",
    "                    y_pred, outputs = nn.NN_test(bert_model_name, model, test_data, label, features)                \n",
    "                                                  \n",
    "                \n",
    "                # evaluate the precision and recall of the predicted repairs\n",
    "                metrics = eva.get_metrics(dpredicted[label].values, \n",
    "                                          dpredicted[label + '_orig'].values,\n",
    "                                          dpredicted[label + '_gs'].values)\n",
    "                result_per_label['metrics'] = metrics\n",
    "                print()\n",
    "            \n",
    "                # result metrics per label\n",
    "                result_per_model[label] = result_per_label\n",
    "                \n",
    "            # result metrics per ML model\n",
    "            all_results[model_name] = result_per_model\n",
    "    \n",
    "            # save the dataset with the predicted repairs for each ML model \n",
    "            file1 = \"./data/{}/repaired/{}_{}_ML_repair_{}.csv\".format(dataFolder, dataFolder, model_name,_with)\n",
    "            print('saveFile', file1)\n",
    "            dpredicted.to_csv(file1, quoting=csv.QUOTE_NONNUMERIC, index=False)\n",
    "            print()\n",
    "\n",
    "        # all records of training and evaluation \n",
    "        strategy_results[_with] = all_results\n",
    "\n",
    "# save training and test accuracy\n",
    "recordFile = \"./results/{}/results_training_ml.json\".format(dataFolder)\n",
    "print('recordFile', recordFile)\n",
    "with open(recordFile, \"w\") as outfile: json.dump(strategy_results, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
