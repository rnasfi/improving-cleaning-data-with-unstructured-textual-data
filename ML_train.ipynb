{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* The purpose of this notebook is to test different ML models in **predicting correct values** of a **dirty data**.\n",
    "* We test the impact on **keeping**/**removing inconsistencies samples** during the **training** phase on the ML model **repair** performence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jar to Python : https://www.jython.org/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as dt\n",
    "import training as tr\n",
    "import preprocessing as tp\n",
    "import model as m\n",
    "import evaluation as eva\n",
    "import utils\n",
    "# import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import csv\n",
    "import json # 2.0.9\n",
    "import config\n",
    "import datetime\n",
    "\n",
    "# import torch # 2.2.0+cu121\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEARNING ALGORITHM\n",
    "algos = config.classifiers #['ann', 'logistic regression l2', 'xgboost', 'multinomial bayesian', ]\n",
    "\n",
    "#LANGUAGE MODEL\n",
    "langs = config.transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: allergens\n",
      "---------------------------\n",
      "feature ingredients\n",
      "---------------------------\n",
      "labels ['nuts', 'milk', 'gluten', 'soy', 'peanut', 'eggs']\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/lab/Documents/rihem/improving-data-cleaning-with-unstructured-data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert_model_name = 'bert-base-multilingual-cased'\n",
    "bert_model_name = 'bert-base-uncased'\n",
    " \n",
    "# Set up parameters\n",
    "num_epochs = 8\n",
    "learning_rate = 2e-5\n",
    "sampling = False\n",
    "# nb of folds\n",
    "splits = 3\n",
    "\n",
    "#overconfidence compensators\n",
    "compensators = [.9,.9,.9]\n",
    "\n",
    "#path to csv files\n",
    "path = os.path.abspath(os.getcwd())\n",
    "\n",
    "# dataset names\n",
    "data_index = 2 #dt.Allergens\n",
    "dataset = config.datasets[data_index] \n",
    "partial_key = dataset['keys'][0]\n",
    "dataName = dataset['data_dir']\n",
    "labels = dataset['labels']\n",
    "feature = dataset['features'][0]\n",
    "lambdac = compensators[data_index]\n",
    "\n",
    "print('dataset:', dataName)\n",
    "print('---------------------------')\n",
    "print('feature', feature)\n",
    "print('---------------------------')\n",
    "print('labels', labels)\n",
    "print('---------------------------')\n",
    "\n",
    "#labels and features\n",
    "dataFolder, labels, features, keys = dt.get_datasetSchema(dataName)\n",
    "partial_key = keys[0]\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training best ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allergens\n"
     ]
    }
   ],
   "source": [
    "print(dataName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: count-vect-xgboost\n",
      "transformer: xgboost classifier: count-vect\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#MODEL\n",
    "model_name = m.get_best_ml(data_index)\n",
    "for l in langs:\n",
    "    if l['name'] in model_name:\n",
    "        lang = l\n",
    "        break\n",
    "for al in algos:\n",
    "    if al['name'] in model_name:\n",
    "        alg = al\n",
    "        break\n",
    "\n",
    "print('model:', model_name)\n",
    "print('transformer:', alg['name'], 'classifier:', lang['name'])\n",
    "print('--------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - parker True - constraints_check False\n",
      "relative path ./data/allergens --before delete (1635, 18)\n",
      "--after delete (1337, 18)\n",
      "label encoders {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies related to: nuts 0\n",
      "dtrain, valid_data (1337, 18) (134, 18)\n",
      "{0: 0.5470668485675307, 1: 1.2570532915360502, 2: 2.6556291390728477}\n",
      "imbalance ratio 0.206 unique_classes [0 1 2] class_counts [733 319 151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:34] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:02,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (134, 18)\n",
      "nuts avg proba 0.8263266921043396 ece 1.8672471791505814\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: milk 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:34] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 33%|███████████████                              | 2/6 [00:00<00:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtrain, valid_data (1337, 18) (134, 18)\n",
      "{0: 0.5440976933514247, 1: 1.474264705882353, 2: 2.0670103092783507}\n",
      "imbalance ratio 0.263 unique_classes [0 1 2] class_counts [737 272 194]\n",
      "\n",
      "\n",
      "valid data (134, 18)\n",
      "milk avg proba 0.8662378370761871 ece 1.6989072216674685\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: gluten 0\n",
      "dtrain, valid_data (1337, 18) (134, 18)\n",
      "{0: 0.5478142076502732, 1: 4.05050505050505, 2: 1.0779569892473118}\n",
      "imbalance ratio 0.135 unique_classes [0 1 2] class_counts [732  99 372]\n",
      "\n",
      "\n",
      "valid data (134, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:34] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 50%|██████████████████████▌                      | 3/6 [00:01<00:01,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gluten avg proba 0.827025318145752 ece 3.704355299472809\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: soy 0\n",
      "dtrain, valid_data (1337, 18) (134, 18)\n",
      "{0: 0.514102564102564, 1: 1.3875432525951557, 2: 2.9925373134328357}\n",
      "imbalance ratio 0.172 unique_classes [0 1 2] class_counts [780 289 134]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:35] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 67%|██████████████████████████████               | 4/6 [00:01<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (134, 18)\n",
      "soy avg proba 0.8736280202865601 ece 2.889617685228586\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: peanut 0\n",
      "dtrain, valid_data (1337, 18) (134, 18)\n",
      "{0: 0.3904576436222006, 1: 2.7094594594594597, 2: 14.321428571428571}\n",
      "imbalance ratio 0.027 unique_classes [0 1 2] class_counts [1027  148   28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:35] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:01<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (134, 18)\n",
      "peanut avg proba 0.8451979815959931 ece 0.48403830314055085\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: eggs 0\n",
      "dtrain, valid_data (1337, 18) (134, 18)\n",
      "{0: 0.3678899082568807, 1: 5.985074626865671, 2: 8.717391304347826}\n",
      "imbalance ratio 0.042 unique_classes [0 1 2] class_counts [1090   67   46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:36] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (134, 18)\n",
      "eggs avg proba 0.8626466274261474 ece 1.658609487581998\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "1 - parker False - constraints_check True\n",
      "relative path ./data/allergens --before delete (1337, 14)\n",
      "--after delete (1333, 14)\n",
      "label encoders {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies related to: nuts 116\n",
      "dtrain, valid_data (945, 14) (95, 14)\n",
      "{0: 0.39297272306981046, 1: 3.586497890295359, 2: 5.666666666666667}\n",
      "imbalance ratio 0.069 unique_classes [0 1 2] class_counts [721  79  50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:36] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:01,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (95, 14)\n",
      "nuts avg proba 0.8472086727619171 ece 0.9391599129885435\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: milk 111\n",
      "dtrain, valid_data (983, 14) (99, 14)\n",
      "{0: 0.41796690307328604, 1: 2.9764309764309766, 2: 3.683333333333333}\n",
      "imbalance ratio 0.113 unique_classes [0 1 2] class_counts [705  99  80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:36] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 33%|███████████████                              | 2/6 [00:00<00:01,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (99, 14)\n",
      "milk avg proba 0.8854178488254547 ece 0.5811132900416851\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: gluten 115\n",
      "dtrain, valid_data (970, 14) (97, 14)\n",
      "{0: 0.4041666666666667, 1: 12.125, 2: 2.255813953488372}\n",
      "imbalance ratio 0.033 unique_classes [0 1 2] class_counts [720  24 129]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:36] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 50%|██████████████████████▌                      | 3/6 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (97, 14)\n",
      "gluten avg proba 0.8719265878200532 ece 1.6712698489427567\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: soy 111\n",
      "dtrain, valid_data (961, 14) (97, 14)\n",
      "{0: 0.38605898123324395, 1: 3.096774193548387, 2: 11.52}\n",
      "imbalance ratio 0.034 unique_classes [0 1 2] class_counts [746  93  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:37] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 67%|██████████████████████████████               | 4/6 [00:01<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (97, 14)\n",
      "soy avg proba 0.8694289326667786 ece 0.35265794745646417\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: peanut 56\n",
      "dtrain, valid_data (1139, 14) (114, 14)\n",
      "{0: 0.3437290409121395, 1: 14.855072463768115, 2: 42.708333333333336}\n",
      "imbalance ratio 0.008 unique_classes [0 1 2] class_counts [994  23   8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:37] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:01<00:00,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (114, 14)\n",
      "peanut avg proba 0.8155520439147949 ece 0.32407428190344945\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "inconsistencies related to: eggs 26\n",
      "dtrain, valid_data (1259, 14) (126, 14)\n",
      "{0: 0.34775936157151627, 1: 11.802083333333334, 2: 25.177777777777777}\n",
      "imbalance ratio 0.014 unique_classes [0 1 2] class_counts [1086   32   15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:33:37] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"clf__learning_rate\", \"clf__max_depth\", \"clf__n_estimators\", \"clf__objective\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:01<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "valid data (126, 14)\n",
      "eggs avg proba 0.8744678020477296 ece 0.3304985645227134\n",
      "+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++\n",
      "\n",
      "recordFile ./results/allergens/results_training_best_ml.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "strategy_results = {}\n",
    "allmodels = {}\n",
    "for i in range(2):\n",
    "    if i == 1:\n",
    "        _parker = \"\"\n",
    "        _with = \"with_constraints\"        \n",
    "        parker = False\n",
    "        constraints_check = True\n",
    "    else:\n",
    "        _parker = \"_parker\"\n",
    "        _with = \"with_parker\"        \n",
    "        parker = True\n",
    "        constraints_check = False\n",
    "    print(i, '- parker', parker, '- constraints_check', constraints_check)\n",
    "    all_results = {}\n",
    "    models = {}\n",
    "    \n",
    "#     start_time = datetime.datetime.now()\n",
    "    \n",
    "    # read training data and valid data\n",
    "    train_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_train.csv\")\n",
    "    train_data = dt.read_data_csv(dataset['data_dir'], 'train', parker) \n",
    "    #utils.load_df(dataset, train_file_path)\n",
    "    # encode non-numerical labels\n",
    "    dtrain, encoders = tp.preprocess(dataset, train_data)\n",
    "    print('label encoders', encoders)\n",
    "    \n",
    "#     feature_processing_time  = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # per_model\n",
    "    result_per_model = {} \n",
    "#     for label in tqdm(labels):\n",
    "    for label in tqdm(labels):\n",
    "        #timestamp\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        #per label       \n",
    "        result_per_label = {}\n",
    "        \n",
    "        # remove the inconsistent rows\n",
    "        if data_index != 4 : dtrain = tr.get_cleaner_train_version(dataName, label, train_data, partial_key)\n",
    "        else: dtrain = train_data.copy() # allergens do need cleaner versssssion\n",
    "        dtrain1, valid_data = train_test_split(dtrain, test_size=0.1)\n",
    "        iter = 0\n",
    "        while len(valid_data[label].unique()) != len(dtrain[label].unique()):\n",
    "            dtrain1, valid_data = train_test_split(dtrain, test_size=0.1)\n",
    "            iter += 1\n",
    "            print(str(iter) + \"th iteration\")\n",
    "        print('dtrain, valid_data', dtrain.shape, valid_data.shape)\n",
    "\n",
    "        y = dtrain1[label].astype(int)\n",
    "        n_class, unique_classes, class_counts, ir = tp.get_class_stats(y)\n",
    "        class_weights = {c: len(y) / (count * n_class) for c, count in zip(unique_classes, class_counts)}\n",
    "        print(class_weights)\n",
    "        \n",
    "        batch_size = dtrain.shape[0]\n",
    "        # Loop through DataFrame in chunks without numpy\n",
    "        for i in range(0, len(dtrain1), batch_size):\n",
    "            if i + batch_size < len(dtrain): batch = dtrain1[i:i + batch_size]\n",
    "            else: batch = dtrain1[i:len(dtrain)]\n",
    "            \n",
    "            # first training\n",
    "            if i == 0:            \n",
    "                model, result_per_label = tr.clf_train(batch, dataset, label, alg, lang, config.root_seed, True)\n",
    "                models[label] = model\n",
    "            else:\n",
    "                print(f\"Batch {(i // batch_size) + 1}:\\n\")\n",
    "                model.fit(batch[feature], batch[label])\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        result_per_label['duration'] = (current_time - start_time).total_seconds()\n",
    "        \n",
    "        # save the model\n",
    "        file_model_name = f\"./models/_{label}_classifier_{model_name}_{_with}.pth\"\n",
    "        pickle.dump(model, open(file_model_name, \"wb\"))\n",
    "        \n",
    "        print()\n",
    "        # save the encoder for each label\n",
    "        if len(encoders)>0:\n",
    "            if len(encoders[label])>0: \n",
    "                result_per_label['encoder'] = encoders[label]\n",
    "        print()\n",
    "            \n",
    "        \n",
    "        # evalutae ML model\n",
    "        print('valid data', valid_data.shape)\n",
    "        #dtest = dt.read_test_csv(dataName, parker)\n",
    "        if alg['name'] != 'ann' and lang['name'] != 'bert':\n",
    "            if len(encoders)>0:\n",
    "                encoder = encoders[label]\n",
    "            else:\n",
    "                encoder = {}\n",
    "            ## load saved model\n",
    "            file_model_name = f\"./models/_{label}_classifier_{model_name}_{_with}.pth\"\n",
    "            with open(file_model_name, 'rb') as f: model = pickle.load(f)                 \n",
    "            \n",
    "            avg_confidence = []\n",
    "            eces = []\n",
    "            outputs = model.predict_proba(valid_data[feature].str.lower())\n",
    "            y_pred = model.predict(valid_data[feature].str.lower())\n",
    "            \n",
    "            y = valid_data[label].values #map(encoder) # encoded\n",
    "    \n",
    "            for i in range(len(set(y))):# iterate over the number of classes\n",
    "                probabilities = outputs[:, i]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "                ece, avg_confidence_in_bin = eva.expected_calibration_error(y, probabilities, i, n_bins=5)\n",
    "                \n",
    "                acf = eva.avg_conf_correct_pred(y, y_pred, probabilities, i)\n",
    "                avg_confidence.append(acf)\n",
    "\n",
    "      \n",
    "        #max_proba = [np.max(p) for p in outputs ]\n",
    "        # compensation for the overconfidence of the model\n",
    "        result_per_label['proba'] = lambdac * sum(avg_confidence)/len(avg_confidence)\n",
    "        print(label, 'avg proba', result_per_label['proba'], 'ece', ece)\n",
    "\n",
    "        result_per_model[label] = result_per_label # for specific model\n",
    "        all_results[model_name] = result_per_model\n",
    "        \n",
    "        print('+++++++++++++++++++++done with this label+++++++++++++++++++++++++++++')\n",
    "        print()\n",
    "\n",
    "      \n",
    "    # sll records of training and evaluation \n",
    "    strategy_results[_with] = all_results\n",
    "    allmodels[_with] = models\n",
    "    \n",
    "strategy_results['lambda'] = lambdac    \n",
    "# save training and test accuracy\n",
    "recordFile = f\"./results/{dataset['data_dir']}/results_training_best_ml.json\"\n",
    "print('recordFile', recordFile)\n",
    "#with open(recordFile, \"w\") as outfile: json.dump(strategy_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'with_parker': {'count-vect-xgboost': {'nuts': {'ir': 0.206,\n",
       "    'duration': 0.399526,\n",
       "    'proba': 0.8263266921043396},\n",
       "   'milk': {'ir': 0.263, 'duration': 0.368092, 'proba': 0.8662378370761871},\n",
       "   'gluten': {'ir': 0.135, 'duration': 0.302608, 'proba': 0.827025318145752},\n",
       "   'soy': {'ir': 0.172, 'duration': 0.302749, 'proba': 0.8736280202865601},\n",
       "   'peanut': {'ir': 0.027, 'duration': 0.333878, 'proba': 0.8451979815959931},\n",
       "   'eggs': {'ir': 0.042, 'duration': 0.329187, 'proba': 0.8626466274261474}}},\n",
       " 'with_constraints': {'count-vect-xgboost': {'nuts': {'ir': 0.069,\n",
       "    'duration': 0.250957,\n",
       "    'proba': 0.8472086727619171},\n",
       "   'milk': {'ir': 0.113, 'duration': 0.257598, 'proba': 0.8854178488254547},\n",
       "   'gluten': {'ir': 0.033, 'duration': 0.251381, 'proba': 0.8719265878200532},\n",
       "   'soy': {'ir': 0.034, 'duration': 0.239615, 'proba': 0.8694289326667786},\n",
       "   'peanut': {'ir': 0.008, 'duration': 0.20171, 'proba': 0.8155520439147949},\n",
       "   'eggs': {'ir': 0.014, 'duration': 0.364554, 'proba': 0.8744678020477296}}},\n",
       " 'lambda': 0.9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordFile = f\"./results/{dataset['data_dir']}/results_training_best_ml.json\"\n",
    "with open(recordFile, \"w\") as outfile: json.dump(strategy_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative path ./data/allergens --before delete (298, 20)\n",
      "--after delete (298, 20)\n",
      "(298, 20)\n",
      "+++++++++++++++++++++Start+++++++++++++++++++++++++++++\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 17 repairs 40 errors 48\n",
      " th 0.85\n",
      "nuts stats: PRECISION, RECALL, F1 (0.42, 0.35, 0.38)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 14 repairs 27 errors 30\n",
      " th 0.89\n",
      "milk stats: PRECISION, RECALL, F1 (0.52, 0.47, 0.49)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 15 repairs 35 errors 34\n",
      " th 0.87\n",
      "gluten stats: PRECISION, RECALL, F1 (0.43, 0.44, 0.43)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 16 repairs 33 errors 34\n",
      " th 0.87\n",
      "soy stats: PRECISION, RECALL, F1 (0.48, 0.47, 0.47)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 6 repairs 18 errors 20\n",
      " th 0.82\n",
      "peanut stats: PRECISION, RECALL, F1 (0.33, 0.3, 0.31)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 1 repairs 9 errors 6\n",
      " th 0.87\n",
      "eggs stats: PRECISION, RECALL, F1 (0.11, 0.17, 0.13)\n",
      "correct_repairs, repairs, errors 85 232 172\n",
      "precision 0.37 recall 0.49\n",
      "recall 0.49\n",
      "F1 0.4216279069767442\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "dtest = dt.read_test_csv(dataName, parker)\n",
    "dtest1 = dtest.copy()\n",
    "print(dtest1.shape)\n",
    "print('+++++++++++++++++++++Start+++++++++++++++++++++++++++++')\n",
    "\n",
    "for a in labels:\n",
    "    # test repaired by parker do not have the following columns: need to fix it!!\n",
    "    if a + '_gs'not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dt.read_gs_csv(dataName)[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_gs'))\n",
    "    # confidence score for each attribute\n",
    "    conf_score = round(strategy_results[_with][model_name][a]['proba'],2)\n",
    "    ## load saved model\n",
    "    file_model_name = f\"./models/_{a}_classifier_{model_name}_{_with}.pth\"\n",
    "    with open(file_model_name, 'rb') as f: model = pickle.load(f) \n",
    "    #model = models[label]\n",
    "\n",
    "    # get the encoder if exists and encode y_orig  y_gs\n",
    "    enc = {}\n",
    "    enc, y_orig, y_gs = tp.encode(encoders, a, dtest1)\n",
    "    print(\"------ done encoding ----------\")      \n",
    "    \n",
    "    # predict the values for the labels to be repaired\n",
    "    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dtest1, a, dataset, enc)\n",
    "    print(\"------ done predicting ----------\")\n",
    "\n",
    "    if a + '_orig' not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dtest1[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_orig')) \n",
    "        print('current columns:', dtest1.columns)\n",
    "\n",
    "    # evaluate on ground truth\n",
    "    y_repair = eva.assign_repair(outputs, y_orig.values, y_pred, conf_score)\n",
    "    # stats\n",
    "    correct_repair, repair, errors = eva.get_stats(y_repair, y_orig.values, y_gs.values)\n",
    "    # metrics\n",
    "    metrics = eva.get_metrics(y_repair, y_orig.values, y_gs.values)\n",
    "    print(' th', conf_score)\n",
    "    print(a, 'stats: PRECISION, RECALL, F1', metrics)\n",
    "\n",
    "    #dtest1[a] = y_pred\n",
    "\n",
    "crs, rs, es = eva.get_all_stats(dtest1, labels)\n",
    "print('correct_repairs, repairs, errors', crs, rs, es)\n",
    "print('precision', round(crs/rs,2), 'recall', round(crs/es,2))\n",
    "if es !=0: \n",
    "    print('recall', round(crs/es,2))\n",
    "    print('F1', 2 * round(crs/rs,2) * round(crs/es,2)/(round(crs/rs,2) + round(crs/es,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n",
      "peanut       0.0\n",
      "peanut_gs    1.0\n",
      "Name: 227, dtype: float64\n",
      "38% kokosnootschilfers|palmolie|rijstdrank poeder|15% amandelen|ruwe rietsuiker|gemalen bourbon vanillestokjes|zeezout\n",
      "4104420182479.0\n",
      "225    0\n",
      "226    0\n",
      "227    0\n",
      "Name: peanut, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "a = random.choice(labels)\n",
    "df1 = dtest.copy()\n",
    "diff = df1[df1[a] != df1[a + '_gs']][[a,a+'_gs']]\n",
    "i = random.choice(diff.index)\n",
    "if diff.shape[0] > 0: \n",
    "    i = random.choice(diff.index)\n",
    "    print(i)\n",
    "    print(diff.loc[i])\n",
    "    print(df1.loc[i, feature])\n",
    "    print(df1.loc[i, partial_key])\n",
    "    print(dtest[dtest[partial_key]== df1.loc[i, partial_key]][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation textual field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = m.get_best_ml(data_index)\n",
    "for l in langs:\n",
    "    if l['name'] in model_name:\n",
    "        lang = l\n",
    "        break\n",
    "transformer = lang\n",
    "trans = transformer[\"fn\"](**transformer[\"fixed_params\"])\n",
    "\n",
    "features = dataset['features'][0]\n",
    "\n",
    "if data_index == 0: ratio = 0.35\n",
    "else: ratio = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate correlation between each text's embedded vector and the label values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies related to: elderly 85\n",
      "data shape initial: 14257\n",
      "data shape now: 14257\n",
      "to save in ./features/_elderly_top_features_tf-idf_with_constraints.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_trans\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_features\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m---> 51\u001b[0m         feature_values \u001b[38;5;241m=\u001b[39m \u001b[43mX_trans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:, i]\n\u001b[1;32m     52\u001b[0m         corr,s  \u001b[38;5;241m=\u001b[39m pearsonr(feature_values, y)\n\u001b[1;32m     53\u001b[0m         correlations\u001b[38;5;241m.\u001b[39mappend(corr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_compressed.py:1062\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     y \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1061\u001b[0m M, N \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39m_swap(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mcsr_todense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and repair data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "strategy_results = {}\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        _with = \"with_constraints\"        \n",
    "        parker = False\n",
    "        constraints_check = True\n",
    "    else:\n",
    "        _with = \"without_constraints\"        \n",
    "        parker = False\n",
    "        constraints_check = False\n",
    "    print(i, '- constraints_check', constraints_check)\n",
    "    all_results = {}\n",
    "    decoders = {}\n",
    "    \n",
    "    _parker = \"\"\n",
    "    if parker: _parker = \"_parker\"\n",
    "\n",
    "    # read training data and test data\n",
    "    train_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_train.csv\")\n",
    "    test_file_path = utils.get_dir(dataset, file=dataset['data_dir'] + _parker + \"_test.csv\")\n",
    "    train_data = utils.load_df(dataset, train_file_path)\n",
    "    test_data = utils.load_df(dataset, test_file_path)\n",
    "    \n",
    "    # encode non-numerical labels\n",
    "    train_data, encoders = tp.preprocess(dataset, train_data)\n",
    "    dpredicted = test_data.copy()\n",
    "\n",
    "    # nlp transformer\n",
    "    for lang in langs:\n",
    "        # classifiers\n",
    "        for alg in algos:\n",
    "            # set a dataframe as repaired data\n",
    "            dpredicted = test_data.copy()\n",
    "            model_name = m.get_modelName(lang['name'], alg['name'])            \n",
    "            print(model_name)\n",
    "            print('---------------------------')\n",
    "\n",
    "            # skip to next model\n",
    "            if (alg['name'] == 'ann' and lang['name'] != 'bert') or (alg['name'] != 'ann' and lang['name'] == 'bert'):\n",
    "                continue\n",
    "            \n",
    "            result_per_model = {}\n",
    "            for label in labels:\n",
    "                file_model_name = f\"./models/_{label}_classifier_{model_name}_{_with}.pth\"\n",
    "                result_per_label = {}\n",
    "                                \n",
    "                # get a cleaner version of the training dataset\n",
    "                if  constraints_check:\n",
    "                    dtrain = tr.get_cleaner_train_version(dataName, label, train_data, partial_key)\n",
    "                \n",
    "                n_class, unique_classes, class_counts, ir = tp.get_class_stats(dtrain[label].values)\n",
    "                \n",
    "                # ML training\n",
    "                if alg['name'] != 'ann' and lang['name'] != 'bert':\n",
    "                    model, result_per_label = tr.clf_train(dtrain, dataset, label, alg, lang, config.root_seed, False)\n",
    "                    # save the ML-based pipeline\n",
    "                    pickle.dump(model, open(file_model_name, \"wb\"))                \n",
    "\n",
    "                # ML model based on BERT and Neural Network\n",
    "                if alg['name'] == 'ann' and lang['name'] == 'bert':\n",
    "                    model, epochs = nn.NN_train(dtrain, label, ir, unique_classes, bert_model_name, learning_rate, num_epochs, features, config.root_seed)\n",
    "                    result_per_label['epochs'] = epochs\n",
    "                    # save the ML-based pipeline\n",
    "                    torch.save(model, file_model_name)\n",
    "                    result_per_label['imbalance ratio'] = ir\n",
    "                \n",
    "                \n",
    "                # evalutae ML model\n",
    "                print('test data', test_data.shape, dpredicted.shape)\n",
    "                if alg['name'] != 'ann' and lang['name'] != 'bert':\n",
    "                    if len(encoders)>0:\n",
    "                        encoder = encoders[label]\n",
    "                    else:\n",
    "                        encoder = {}\n",
    "                    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dpredicted, label, dataset, encoder)\n",
    "                    #dpredicted[label + '_orig'] = test_data[label].values \n",
    "                    #dpredicted[label] = y_pred\n",
    "                    print('dpredicted.columns', dpredicted.columns)\n",
    "                    # evaluate the accuracy of the predicted repairs                \n",
    "                    result_per_label['test accuracy'] = accuracy\n",
    "                    print(label, f\"Test Accuracy: {accuracy:.4f}\")                     \n",
    "                if alg['name'] == 'ann' and lang['name'] == 'bert': \n",
    "                    y_pred, outputs = nn.NN_test(bert_model_name, model, test_data, label, features)                \n",
    "                                                  \n",
    "                \n",
    "                # evaluate the precision and recall of the predicted repairs\n",
    "                metrics = eva.get_metrics(dpredicted[label].values, \n",
    "                                          dpredicted[label + '_orig'].values,\n",
    "                                          dpredicted[label + '_gs'].values)\n",
    "                result_per_label['metrics'] = metrics\n",
    "                print()\n",
    "            \n",
    "                # result metrics per label\n",
    "                result_per_model[label] = result_per_label\n",
    "                \n",
    "            # result metrics per ML model\n",
    "            all_results[model_name] = result_per_model\n",
    "    \n",
    "            # save the dataset with the predicted repairs for each ML model \n",
    "            file1 = \"./data/{}/repaired/{}_{}_ML_repair_{}.csv\".format(dataFolder, dataFolder, model_name,_with)\n",
    "            print('saveFile', file1)\n",
    "            dpredicted.to_csv(file1, quoting=csv.QUOTE_NONNUMERIC, index=False)\n",
    "            print()\n",
    "\n",
    "        # all records of training and evaluation \n",
    "        strategy_results[_with] = all_results\n",
    "\n",
    "# save training and test accuracy\n",
    "recordFile = \"./results/{}/results_training_ml.json\".format(dataFolder)\n",
    "print('recordFile', recordFile)\n",
    "with open(recordFile, \"w\") as outfile: json.dump(strategy_results, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
