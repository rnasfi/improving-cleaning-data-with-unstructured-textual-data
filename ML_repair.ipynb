{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install -U scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.4\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import dataset as dt\n",
    "import training as tr\n",
    "import preprocessing as tp\n",
    "import model as m\n",
    "import evaluation as eva\n",
    "import utils\n",
    "import config\n",
    "# import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: tf-idf-xgboost\n",
      "---------------------------\n",
      "dataset: trials_population\n",
      "---------------------------\n",
      "feature inclusion\n",
      "---------------------------\n",
      "labels ['elderly', 'adults', 'adolescents', 'children', 'female', 'male', 'healthy_volunteers']\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# dataset names\n",
    "data_index = 1\n",
    "if data_index == 2 : j = 0\n",
    "else: j = 1\n",
    "dataset = config.datasets[data_index] #dt.Allergens\n",
    "partial_key = dataset['keys'][0]\n",
    "labels = dataset['labels']\n",
    "feature = dataset['features'][0]\n",
    "dataName = dataset['data_dir']\n",
    "\n",
    "#LEARNING ALGORITHM\n",
    "alg = config.classifier #'multinomial bayesian' #'xgboost'\n",
    "#LANGUAGE MODEL\n",
    "lang = config.transformers[j] #'tf-idf'\n",
    "#MODEL\n",
    "model_name = m.get_best_ml(data_index) #get_modelName(lang[\"name\"], alg[\"name\"])\n",
    "print('model:', model_name)\n",
    "print('---------------------------')\n",
    "parker =  False #True #False #\n",
    "\n",
    "seed = 42\n",
    "\n",
    "print('dataset:', dataName)\n",
    "print('---------------------------')\n",
    "print('feature', feature)\n",
    "print('---------------------------')\n",
    "print('labels', labels)\n",
    "print('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not parker:\n",
    "    _parker = \"\"\n",
    "    _with = \"with_constraints\"\n",
    "else:\n",
    "    _parker = \"_parker\"\n",
    "    _with = 'with_parker'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configFile ./results/trials_population/results_training_best_ml.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configFile = f\"./results/{dataset['data_dir']}/results_training_best_ml.json\"\n",
    "\n",
    "print('configFile', configFile)\n",
    "\n",
    "f = open(configFile)\n",
    "\n",
    "records = json.load(f)\n",
    "\n",
    "encoder = {} \n",
    "for label in labels:\n",
    "    if 'encoder' in records[_with][model_name][label]:\n",
    "        encoder[label] = records[_with][model_name][label]['encoder']\n",
    "    \n",
    "f.close()\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test robustness of the trained model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from evaluator import evaluator as evals\n",
    "\n",
    "ev = evals.Evaluator(dataset, parker, model_name)\n",
    "ev.test_model_robustness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test the trained model and save the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative path ./data/trials_population --before delete (1269, 23)\n",
      "--after delete (1269, 23)\n",
      "a= elderly [0 1]\n",
      "correct repair 24 repairs 68 errors 34\n",
      "elderly stats: PRECISION, RECALL, F1 (0.35, 0.71, 0.47)\n",
      "----------------------------------------\n",
      "correct_repairs, repairs, errors 24 68 34\n",
      "precision 0.35\n",
      "recall 0.71\n",
      "F1 0.4688679245283018\n"
     ]
    }
   ],
   "source": [
    "dtest = dt.read_test_csv(dataName, parker)\n",
    "\n",
    "for a in labels:\n",
    "    if a + '_gs'not in dtest.columns:\n",
    "        dtest = dtest.merge(dt.read_gs_csv(dataName)[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_gs'))\n",
    "\n",
    "    ## load saved model\n",
    "    file_model_name = f\"./models/_{a}_classifier_{model_name}_{_with}.pth\"\n",
    "    with open(file_model_name, 'rb') as f: model = pickle.load(f)   \n",
    "\n",
    "\n",
    "    # predict the values for the labels to be repaired\n",
    "    enc, y_orig, y_gs = tp.encode(encoder, a, dtest)\n",
    "    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dtest, a, dataset, enc)\n",
    "\n",
    "    print('a=', a, dtest[a].unique())\n",
    "    # metrics\n",
    "    metrics = eva.get_metrics(y_pred, y_orig.values, y_gs.values)\n",
    "    print(a, 'stats: PRECISION, RECALL, F1', metrics)\n",
    "    break\n",
    "\n",
    "print('----------------------------------------')\n",
    "crs, rs, es = eva.get_all_stats(dtest, [a])\n",
    "print('correct_repairs, repairs, errors', crs, rs, es)\n",
    "print('precision', round(crs/rs,2))\n",
    "if es !=0: \n",
    "    print('recall', round(crs/es,2))\n",
    "    print('F1', 2 * round(crs/rs,2) * round(crs/es,2)/(round(crs/rs,2) + round(crs/es,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save last repaired datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File to be saved: data/trials_population/repaired/trials_population_tf-idf-xgboost_ML_repair_with_constraints.csv\n"
     ]
    }
   ],
   "source": [
    "if not parker:\n",
    "    a = labels[0]\n",
    "    dtest[[a, a+'_gs']][dtest[a] != dtest[a+'_gs']].shape, dtest.shape\n",
    "file = f\"data/{dataset['data_dir']}/repaired/{dataset['data_dir']}_{model_name}_ML_repair_{_with}.csv\"\n",
    "\n",
    "print('File to be saved:', file)\n",
    "#dtest.to_csv(file, quoting=csv.QUOTE_NONNUMERIC, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative path ./data/trials_population --before delete (1269, 23)\n",
      "--after delete (1269, 23)\n",
      "(1269, 23)\n",
      "+++++++++++++++++++++Start+++++++++++++++++++++++++++++\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 16 repairs 20 errors 34\n",
      " th 0.95\n",
      "elderly stats: PRECISION, RECALL, F1 (0.8, 0.47, 0.59)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 4 repairs 9 errors 12\n",
      " th 0.99\n",
      "adults stats: PRECISION, RECALL, F1 (0.44, 0.33, 0.38)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 207 repairs 210 errors 210\n",
      " th 0.99\n",
      "adolescents stats: PRECISION, RECALL, F1 (0.99, 0.99, 0.99)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 207 repairs 212 errors 217\n",
      " th 0.99\n",
      "children stats: PRECISION, RECALL, F1 (0.98, 0.95, 0.96)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 1 repairs 1 errors 1\n",
      " th 1.0\n",
      "female stats: PRECISION, RECALL, F1 (1.0, 1.0, 1.0)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 0 repairs 0 errors 5\n",
      " th 1.0\n",
      "male stats: PRECISION, RECALL, F1 (0, 0.0, 0)\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 17 repairs 17 errors 17\n",
      " th 1.0\n",
      "healthy_volunteers stats: PRECISION, RECALL, F1 (1.0, 1.0, 1.0)\n",
      "correct_repairs, repairs, errors 472 692 496\n",
      "precision 0.68 recall 0.95\n",
      "recall 0.95\n",
      "F1 0.792638036809816\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "dtest = dt.read_test_csv(dataName, parker)\n",
    "dtest1 = dtest.copy()\n",
    "print(dtest1.shape)\n",
    "print('+++++++++++++++++++++Start+++++++++++++++++++++++++++++')\n",
    "\n",
    "for a in labels:\n",
    "    # test repaired by parker do not have the following columns: need to fix it!!\n",
    "    if a + '_gs'not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dt.read_gs_csv(dataName)[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_gs'))\n",
    "    # confidence score for each attribute\n",
    "    conf_score = round(records[_with][model_name][a]['proba'],2)\n",
    "    ## load saved model\n",
    "    file_model_name = f\"./models/_{a}_classifier_{model_name}_{_with}.pth\"\n",
    "    with open(file_model_name, 'rb') as f: model = pickle.load(f)   \n",
    "\n",
    "    # get the encoder if exists and encode y_orig  y_gs\n",
    "    enc = {}\n",
    "    enc, y_orig, y_gs = tp.encode(encoder, a, dtest1)\n",
    "    print(\"------ done encoding ----------\")      \n",
    "    \n",
    "    # predict the values for the labels to be repaired\n",
    "    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dtest1, a, dataset, enc)\n",
    "    print(\"------ done predicting ----------\")\n",
    "\n",
    "    if a + '_orig' not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dtest1[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_orig')) \n",
    "        print('current columns:', dtest1.columns)\n",
    "\n",
    "    # evaluate on ground truth\n",
    "    y_repair = eva.assign_repair(outputs, y_orig.values, y_pred, conf_score)\n",
    "    # stats\n",
    "    correct_repair, repair, errors = eva.get_stats(y_repair, y_orig.values, y_gs.values)\n",
    "    # metrics\n",
    "    metrics = eva.get_metrics(y_repair, y_orig.values, y_gs.values)\n",
    "    print(' th', conf_score)\n",
    "    print(a, 'stats: PRECISION, RECALL, F1', metrics)\n",
    "\n",
    "    #dtest1[a] = y_pred\n",
    "\n",
    "crs, rs, es = eva.get_all_stats(dtest1, labels)\n",
    "print('correct_repairs, repairs, errors', crs, rs, es)\n",
    "print('precision', round(crs/rs,2), 'recall', round(crs/es,2))\n",
    "if es !=0: \n",
    "    print('recall', round(crs/es,2))\n",
    "    print('F1', 2 * round(crs/rs,2) * round(crs/es,2)/(round(crs/rs,2) + round(crs/es,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File to be saved: ./data/trials_population/repaired/trials_population_tf-idf-xgboost_ML_repair_with_constraints_threshold.csv test data (1269, 30)\n"
     ]
    }
   ],
   "source": [
    "file = f\"./data/{dataset['data_dir']}/repaired/{dataset['data_dir']}_{model_name}_ML_repair_{_with}_threshold.csv\"\n",
    "print('File to be saved:', file, 'test data', dtest.shape)\n",
    "file2 = f\"data/{dataset['data_dir']}/{dataset['data_dir']}.csv\"\n",
    "\n",
    "dtest.to_csv(file, quoting=csv.QUOTE_NONNUMERIC, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to be remved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_additional_rows(dataset, data, additional_file_dir):\n",
    "    \"\"\" returns a (allergen) dataset that can be added to evaluate the predictions\n",
    "        seems useful to fed to Parker engine\n",
    "        specific to allergens dataset due to using keys.\n",
    "\n",
    "    Args:\n",
    "        test_file_dir (String): directory to the test dataset\n",
    "        additional_file_dir (String): directory to the dataset where the valuable rows can be added\n",
    "        quote (Boolean): specifies whther the textual attributes are wrapped by quotes or not\n",
    "\n",
    "    Returns:\n",
    "        to_be_added (DataFrame): dataframe of rows that can be added to the predictions\n",
    "    \"\"\"   \n",
    "    \n",
    "    # read test file\n",
    "    test = data.copy() #pd.read_csv(test_file_dir, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print('test size', test.shape)\n",
    "\n",
    "    # read the dataset allergens from ledc gitlab\n",
    "    more_rows = pd.read_csv(additional_file_dir, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print('test file', additional_file_dir)\n",
    "    print('test size', more_rows.shape)\n",
    "    \n",
    "    overlap = more_rows[(more_rows[dataset['keys'][0]].isin(test[dataset['keys'][0]])) & (more_rows[dataset['keys'][1]].isin(test[dataset['keys'][1]]))]\n",
    "    print('overlap', overlap.shape)\n",
    "    \n",
    "    indices = [i for i in more_rows.index if i not in overlap.index]\n",
    "    nbs = round(len(indices)*.78)\n",
    "    to_be_added = more_rows.loc[random.sample(indices,nbs)]\n",
    "    return to_be_added\n",
    "\n",
    "def put_more_into_test_dataset(dataset, file1, file2, repaired, save_file):\n",
    "    no_overlap = get_additional_rows(dataset, repaired, file2)\n",
    "    co = [a for a in repaired.columns if a in no_overlap.columns]\n",
    "    alles = pd.concat([repaired, no_overlap[co]], ignore_index=True)\n",
    "\n",
    "    alles.to_csv(save_file, index=False, quoting=csv.QUOTE_NONNUMERIC)    \n",
    "    return alles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "if not parker and data_index == 2:\n",
    "    # add more rows to be repaired in Parker engine\n",
    "    new_data = put_more_into_test_dataset(dataset, file, file2, dtest, file)\n",
    "    print('new shape', new_data.shape)\n",
    "    new_data.to_csv(file, quoting=csv.QUOTE_NONNUMERIC, index=False)\n",
    "else:\n",
    "    #file = f\"./data/{dataset['data_dir']}/repaired/{dataset['data_dir']}_{model_name}_ML_repair_{_with}_threshold.csv\"\n",
    "    #print('File to be saved:', file)\n",
    "    dtest.to_csv(file, quoting=csv.QUOTE_NONNUMERIC, index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = f\"./data/{dataset['data_dir']}/{dataset['data_dir']}_test.csv\"\n",
    "#file = f\"./data/{dataset['data_dir']}/repaired/{dataset['data_dir']}_{model_name}_ML_repair_{_with}_threshold.csv\"\n",
    "print('File to be saved:', file)\n",
    "file2 = f\"data/{dataset['data_dir']}/{dataset['data_dir']}_parker.csv\"\n",
    "file2 = f\"./data/{dataset['data_dir']}/{dataset['data_dir']}_test1.csv\"\n",
    "dtest = pd.read_csv(file)\n",
    "\n",
    "dtest.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# holoclean !!\n",
    "new_data = put_more_into_test_dataset(dataset, file, file2, dtest, file3)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = f\"./data/{dataset['data_dir']}/repaired/{dataset['data_dir']}_{model_name}_ML_repair_{_with}_threshold1.csv\"\n",
    "print('File to be saved:', file)\n",
    "if not parker: # and data_index == 2\n",
    "    # add more rows to be repaired in Parker engine\n",
    "    new_data = put_more_into_test_dataset(dataset, file, file2, dtest, file)\n",
    "    print('new shape', new_data.shape)\n",
    "else:\n",
    "    #file = f\"./data/{dataset['data_dir']}/repaired/{dataset['data_dir']}_{model_name}_ML_repair_{_with}_threshold.csv\"\n",
    "    #print('File to be saved:', file)\n",
    "    dtest.to_csv(file, quoting=csv.QUOTE_NONNUMERIC, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative path ./data/trials_population --before delete (1269, 23)\n",
      "--after delete (1269, 23)\n",
      "+++++++++++++++++++++Start+++++++++++++++++++++++++++++\n",
      "(1269, 23)\n",
      "label elderly avg proba 0.95 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 0.95, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 24 repairs 68 errors 34\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.35, 0.71, 0.47)\n",
      "correct repair 24 repairs 68 errors 34\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.35, 0.71, 0.47)\n",
      "correct repair 24 repairs 68 errors 34\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.35, 0.71, 0.47)\n",
      "correct repair 24 repairs 68 errors 34\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.35, 0.71, 0.47)\n",
      "correct repair 24 repairs 52 errors 34\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (0.46, 0.71, 0.56)\n",
      "correct repair 23 repairs 34 errors 34\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (0.68, 0.68, 0.68)\n",
      "correct repair 16 repairs 20 errors 34\n",
      " th 0.95\n",
      "stats: correct_repairs, repairs, errors (0.8, 0.47, 0.59)\n",
      "correct repair 4 repairs 4 errors 34\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (1.0, 0.12, 0.21)\n",
      "+++++++++++++++++++++done with elderly+++++++++++++++++++++++++++++\n",
      "\n",
      "label adults avg proba 0.99 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 0.99, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 11 repairs 47 errors 12\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.23, 0.92, 0.37)\n",
      "correct repair 11 repairs 47 errors 12\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.23, 0.92, 0.37)\n",
      "correct repair 11 repairs 47 errors 12\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.23, 0.92, 0.37)\n",
      "correct repair 11 repairs 47 errors 12\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.23, 0.92, 0.37)\n",
      "correct repair 11 repairs 43 errors 12\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (0.26, 0.92, 0.41)\n",
      "correct repair 11 repairs 24 errors 12\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (0.46, 0.92, 0.61)\n",
      "correct repair 4 repairs 9 errors 12\n",
      " th 0.99\n",
      "stats: correct_repairs, repairs, errors (0.44, 0.33, 0.38)\n",
      "correct repair 4 repairs 4 errors 12\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (1.0, 0.33, 0.5)\n",
      "+++++++++++++++++++++done with adults+++++++++++++++++++++++++++++\n",
      "\n",
      "label adolescents avg proba 0.99 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 0.99, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 207 repairs 251 errors 210\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.82, 0.99, 0.9)\n",
      "correct repair 207 repairs 251 errors 210\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.82, 0.99, 0.9)\n",
      "correct repair 207 repairs 251 errors 210\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.82, 0.99, 0.9)\n",
      "correct repair 207 repairs 251 errors 210\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.82, 0.99, 0.9)\n",
      "correct repair 207 repairs 233 errors 210\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (0.89, 0.99, 0.94)\n",
      "correct repair 207 repairs 222 errors 210\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (0.93, 0.99, 0.96)\n",
      "correct repair 207 repairs 210 errors 210\n",
      " th 0.99\n",
      "stats: correct_repairs, repairs, errors (0.99, 0.99, 0.99)\n",
      "correct repair 207 repairs 209 errors 210\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (0.99, 0.99, 0.99)\n",
      "+++++++++++++++++++++done with adolescents+++++++++++++++++++++++++++++\n",
      "\n",
      "label children avg proba 0.99 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 0.99, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 211 repairs 241 errors 217\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.88, 0.97, 0.92)\n",
      "correct repair 211 repairs 241 errors 217\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.88, 0.97, 0.92)\n",
      "correct repair 211 repairs 241 errors 217\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.88, 0.97, 0.92)\n",
      "correct repair 211 repairs 241 errors 217\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.88, 0.97, 0.92)\n",
      "correct repair 209 repairs 236 errors 217\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (0.89, 0.96, 0.92)\n",
      "correct repair 208 repairs 226 errors 217\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (0.92, 0.96, 0.94)\n",
      "correct repair 207 repairs 212 errors 217\n",
      " th 0.99\n",
      "stats: correct_repairs, repairs, errors (0.98, 0.95, 0.96)\n",
      "correct repair 207 repairs 209 errors 217\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (0.99, 0.95, 0.97)\n",
      "+++++++++++++++++++++done with children+++++++++++++++++++++++++++++\n",
      "\n",
      "label female avg proba 1.0 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 1 repairs 46 errors 1\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.02, 1.0, 0.04)\n",
      "correct repair 1 repairs 46 errors 1\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.02, 1.0, 0.04)\n",
      "correct repair 1 repairs 46 errors 1\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.02, 1.0, 0.04)\n",
      "correct repair 1 repairs 46 errors 1\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.02, 1.0, 0.04)\n",
      "correct repair 1 repairs 43 errors 1\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (0.02, 1.0, 0.04)\n",
      "correct repair 1 repairs 38 errors 1\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (0.03, 1.0, 0.06)\n",
      "correct repair 1 repairs 1 errors 1\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (1.0, 1.0, 1.0)\n",
      "correct repair 1 repairs 1 errors 1\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (1.0, 1.0, 1.0)\n",
      "+++++++++++++++++++++done with female+++++++++++++++++++++++++++++\n",
      "\n",
      "label male avg proba 1.0 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 1 repairs 21 errors 5\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.05, 0.2, 0.08)\n",
      "correct repair 1 repairs 21 errors 5\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.05, 0.2, 0.08)\n",
      "correct repair 1 repairs 21 errors 5\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.05, 0.2, 0.08)\n",
      "correct repair 1 repairs 21 errors 5\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.05, 0.2, 0.08)\n",
      "correct repair 1 repairs 20 errors 5\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (0.05, 0.2, 0.08)\n",
      "correct repair 1 repairs 19 errors 5\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (0.05, 0.2, 0.08)\n",
      "correct repair 0 repairs 0 errors 5\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (0, 0.0, 0)\n",
      "correct repair 0 repairs 0 errors 5\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (0, 0.0, 0)\n",
      "+++++++++++++++++++++done with male+++++++++++++++++++++++++++++\n",
      "\n",
      "label healthy_volunteers avg proba 1.0 ths [0, 0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.0]\n",
      "------ done encoding ----------\n",
      "------ done predicting ----------\n",
      "correct repair 17 repairs 18 errors 17\n",
      " th 0\n",
      "stats: correct_repairs, repairs, errors (0.94, 1.0, 0.97)\n",
      "correct repair 17 repairs 18 errors 17\n",
      " th 0.0\n",
      "stats: correct_repairs, repairs, errors (0.94, 1.0, 0.97)\n",
      "correct repair 17 repairs 18 errors 17\n",
      " th 0.2\n",
      "stats: correct_repairs, repairs, errors (0.94, 1.0, 0.97)\n",
      "correct repair 17 repairs 18 errors 17\n",
      " th 0.4\n",
      "stats: correct_repairs, repairs, errors (0.94, 1.0, 0.97)\n",
      "correct repair 17 repairs 17 errors 17\n",
      " th 0.6000000000000001\n",
      "stats: correct_repairs, repairs, errors (1.0, 1.0, 1.0)\n",
      "correct repair 17 repairs 17 errors 17\n",
      " th 0.8\n",
      "stats: correct_repairs, repairs, errors (1.0, 1.0, 1.0)\n",
      "correct repair 17 repairs 17 errors 17\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (1.0, 1.0, 1.0)\n",
      "correct repair 17 repairs 17 errors 17\n",
      " th 1.0\n",
      "stats: correct_repairs, repairs, errors (1.0, 1.0, 1.0)\n",
      "+++++++++++++++++++++done with healthy_volunteers+++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++more sources+++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "dtest = dt.read_test_csv(dataName, parker)\n",
    "\n",
    "print('+++++++++++++++++++++Start+++++++++++++++++++++++++++++')\n",
    "\n",
    "dtest1 = dtest.copy()\n",
    "print(dtest1.shape)\n",
    "\n",
    "for a in labels:\n",
    "    # test repaired by parker do not have the following columns: need to fix it!!\n",
    "    if a + '_gs'not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dt.read_gs_csv(dataName)[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_gs'))\n",
    "\n",
    "\n",
    "    avg_proba = round(records[_with][model_name][a]['proba'],2)\n",
    "\n",
    "    thresholds = [0, avg_proba] + [th for th in np.arange(0, 1.1, 0.2)]\n",
    "\n",
    "    ## load saved model\n",
    "    file_model_name = f\"./models/_{a}_classifier_{model_name}_{_with}.pth\"\n",
    "    with open(file_model_name, 'rb') as f: model = pickle.load(f)   \n",
    "\n",
    "    thresholds.sort()\n",
    "    print('label',a, 'avg proba', avg_proba, 'ths', thresholds)\n",
    "\n",
    "    # get the encoder if exists and encode y_orig  y_gs\n",
    "    enc = {}\n",
    "    enc, y_orig, y_gs = tp.encode(encoder, a, dtest1)\n",
    "    print(\"------ done encoding ----------\")      \n",
    "    \n",
    "    # predict the values for the labels to be repaired\n",
    "    y_pred, outputs, dtest, accuracy = tr.clf_test(model, dtest1, a, dataset, enc)\n",
    "    print(\"------ done predicting ----------\")\n",
    "\n",
    "    if a + '_orig' not in dtest1.columns:\n",
    "        dtest1 = dtest1.merge(dtest1[[partial_key, a ]], \n",
    "                              how='inner', on=partial_key, suffixes=('', '_orig')) \n",
    "        print('current columns:', dtest1.columns)\n",
    "    \n",
    "    repairs = []\n",
    "    correct_repairs = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    # evaluate on ground truth\n",
    "    for th in  thresholds: \n",
    "        y_repair = eva.assign_repair(outputs, y_orig.values, y_pred, th)\n",
    "        # stats\n",
    "        correct_repair, repair, errors = eva.get_stats(y_repair, y_orig.values, y_gs.values)\n",
    "        correct_repairs.append(correct_repair)\n",
    "        repairs.append(repair)\n",
    "\n",
    "        # metrics\n",
    "        metrics = eva.get_metrics(y_repair, y_orig.values, y_gs.values)\n",
    "        recalls.append(round(metrics[1],2))\n",
    "        precisions.append(round(metrics[0],2))\n",
    "        f1s.append(round(metrics[2],2))\n",
    "\n",
    "        print(' th', th, )\n",
    "        print('stats: correct_repairs, repairs, errors', metrics)\n",
    "\n",
    "    stats[a] = {\"errors\": errors, \"avg_proba\": avg_proba,\\\n",
    "                \"threshold\": [round(th,2) for th in thresholds], 'repairs': repairs, 'correct_repairs': correct_repairs, \"precision\": precisions, \"recall\": recalls, \"F-1\": f1s}\n",
    "    print(f\"+++++++++++++++++++++done with {a}+++++++++++++++++++++++++++++\")\n",
    "    print()\n",
    "#         break\n",
    "print('+++++++++++++++++++++more sources+++++++++++++++++++++++++++++')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save statistics of models performences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/trials_population/trials_population_stats_tf-idf-xgboost_with_constraints.json\n"
     ]
    }
   ],
   "source": [
    "statFile = f\"./results/{dataset['data_dir']}/{dataset['data_dir']}_stats_{model_name}_{_with}.json\"\n",
    "print(statFile)\n",
    "with open(statFile, \"w\") as outfile: \n",
    "        json.dump(stats, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1269, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check an instance of repairing an erroneous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609\n",
      "elderly       0.0\n",
      "elderly_gs    1.0\n",
      "Name: 609, dtype: float64\n",
      "Patient must be  18 and  90 years of age   Female patients can participate if they are surgically sterile or completed menopause or females capable of having children and agree not to attempt pregnancy while receiving IV study therapy and for a period of 7 days after   Patient has a ceftazidime resistant Gram negative pathogen that was isolated from an appropriate culture within 5 days prior to study entry  ie  within 5 days prior to Screening  the study qualifying culture   which was determined to be the causative agent of the entry infection\n",
      "2012-000726-21\n",
      "607    0\n",
      "608    0\n",
      "609    0\n",
      "610    0\n",
      "611    0\n",
      "Name: elderly, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a = random.choice(labels)\n",
    "df1 = dtest.copy()\n",
    "diff = df1[df1[a] != df1[a + '_gs']][[a,a+'_gs']]\n",
    "i = random.choice(diff.index)\n",
    "if diff.shape[0] > 0: \n",
    "    i = random.choice(diff.index)\n",
    "    print(i)\n",
    "    print(diff.loc[i])\n",
    "    print(df1.loc[i, feature])\n",
    "    print(df1.loc[i, partial_key])\n",
    "    print(dtest[dtest[partial_key]== df1.loc[i, partial_key]][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
